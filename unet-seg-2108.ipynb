{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nibabel\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/7f/d3c29792fae50ef4f1f8f87af8a94d5d9fe76550b86ebcf8a251110169d8/nibabel-3.2.0-py3-none-any.whl (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from nibabel) (1.19.4)\n",
      "Requirement already satisfied: packaging>=14.3 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from nibabel) (20.4)\n",
      "Requirement already satisfied: six in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from packaging>=14.3->nibabel) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from packaging>=14.3->nibabel) (2.4.7)\n",
      "Installing collected packages: nibabel\n",
      "Successfully installed nibabel-3.2.0\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.3b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorlayer==1.11.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/6f/6a763a00303f62154eadb5ec272b2b2b91dd3305df09b46a157a4b60b90b/tensorlayer-1.11.0-py2.py3-none-any.whl (316kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 18.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting wrapt<1.11,>=1.10 (from tensorlayer==1.11.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
      "Collecting numpy<1.16,>=1.14 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<2.20,>=2.19 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 42.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting progressbar2<3.39,>=3.38 (from tensorlayer==1.11.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
      "Collecting scikit-image<0.15,>=0.14 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/78/cfb15cdb3f63eea16946a42d0dbf7ef17be79d30858aa8efd5f6757bd106/scikit_image-0.14.5-cp36-cp36m-manylinux1_x86_64.whl (25.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.4MB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml<4.3,>=4.2 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/b6/0ca1685d450e07bf329601b2aaf552e71356625a798b15147250db8e9329/lxml-4.2.6-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 12.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting scikit-learn<0.21,>=0.19 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/5b/5da31a6572dc6b7b2846a7cfcbe2e060a0e6af0e1059a6516965e40371b7/scikit_learn-0.20.4-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 13.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting scipy<1.2,>=1.1 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 31.2MB 1.9MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting matplotlib<3.1,>=2.2 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/69/f5e05f578585ed9935247be3788b374f90701296a70c8871bcd6d21edb00/matplotlib-3.0.3-cp36-cp36m-manylinux1_x86_64.whl (13.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.0MB 5.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio<2.5,>=2.3 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 19.5MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting tqdm<4.28,>=4.23 (from tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/33/6d8bd6a7c4238f383426b7593bf05bfd6d9e1f10c3084b56c0f14d973754/tqdm-4.27.0-py2.py3-none-any.whl (44kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 32.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from requests<2.20,>=2.19->tensorlayer==1.11.0) (2020.11.8)\n",
      "Collecting idna<2.8,>=2.5 (from requests<2.20,>=2.19->tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 37.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from requests<2.20,>=2.19->tensorlayer==1.11.0) (3.0.4)\n",
      "Collecting urllib3<1.24,>=1.21.1 (from requests<2.20,>=2.19->tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 50.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting python-utils>=2.3.0 (from progressbar2<3.39,>=3.38->tensorlayer==1.11.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/ff/623dfa533f3277199957229f053fdb2c73a9c18048680e1899c9a5c95e6b/python_utils-2.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from progressbar2<3.39,>=3.38->tensorlayer==1.11.0) (1.15.0)\n",
      "Collecting pillow>=4.3.0 (from scikit-image<0.15,>=0.14->tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 20.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=1.8 (from scikit-image<0.15,>=0.14->tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/cd/dc52755d30ba41c60243235460961fc28022e5b6731f16c268667625baea/networkx-2.5-py3-none-any.whl (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 28.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets>=0.4.0 (from scikit-image<0.15,>=0.14->tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/bb/d2b85265ec9fa3c1922210c9393d4cdf7075cc87cce6fe671d7455f80fbc/PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.4MB 11.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle>=0.2.1 (from scikit-image<0.15,>=0.14->tensorlayer==1.11.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from matplotlib<3.1,>=2.2->tensorlayer==1.11.0) (2.4.7)\n",
      "Collecting cycler>=0.10 (from matplotlib<3.1,>=2.2->tensorlayer==1.11.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from matplotlib<3.1,>=2.2->tensorlayer==1.11.0) (2.8.1)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib<3.1,>=2.2->tensorlayer==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/1b/cbd8ae738719b5f41592a12057ef5442e2ed5f5cb5451f8fc7e9f8875a1a/kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.1MB 35.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer==1.11.0) (4.4.2)\n",
      "Building wheels for collected packages: wrapt, imageio\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/azureuser/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
      "  Running setup.py bdist_wheel for imageio ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/azureuser/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
      "Successfully built wrapt imageio\n",
      "Installing collected packages: wrapt, numpy, idna, urllib3, requests, python-utils, progressbar2, pillow, scipy, networkx, cycler, kiwisolver, matplotlib, PyWavelets, cloudpickle, scikit-image, lxml, scikit-learn, imageio, tqdm, tensorlayer\n",
      "  Found existing installation: numpy 1.19.4\n",
      "    Uninstalling numpy-1.19.4:\n",
      "      Successfully uninstalled numpy-1.19.4\n",
      "  Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Found existing installation: urllib3 1.26.2\n",
      "    Uninstalling urllib3-1.26.2:\n",
      "      Successfully uninstalled urllib3-1.26.2\n",
      "  Found existing installation: requests 2.25.0\n",
      "    Uninstalling requests-2.25.0:\n",
      "      Successfully uninstalled requests-2.25.0\n",
      "Successfully installed PyWavelets-1.1.1 cloudpickle-1.6.0 cycler-0.10.0 idna-2.7 imageio-2.4.1 kiwisolver-1.3.1 lxml-4.2.6 matplotlib-3.0.3 networkx-2.5 numpy-1.15.4 pillow-8.0.1 progressbar2-3.38.0 python-utils-2.4.0 requests-2.19.1 scikit-image-0.14.5 scikit-learn-0.20.4 scipy-1.1.0 tensorlayer-1.11.0 tqdm-4.27.0 urllib3-1.23 wrapt-1.10.11\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.3b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorlayer==1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (1.5.9)\n",
      "Requirement already satisfied: urllib3 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from kaggle) (1.23)\n",
      "Requirement already satisfied: python-dateutil in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: python-slugify in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from kaggle) (4.0.1)\n",
      "Requirement already satisfied: slugify in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from kaggle) (0.0.1)\n",
      "Requirement already satisfied: tqdm in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from kaggle) (4.27.0)\n",
      "Requirement already satisfied: six>=1.10 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: certifi in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from kaggle) (2020.11.8)\n",
      "Requirement already satisfied: requests in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from kaggle) (2.19.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from requests->kaggle) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages (from requests->kaggle) (3.0.4)\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.3b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gc\n",
    "import pickle\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading brats2018.zip to /media/unet-hdd128\n",
      "100%|█████████████████████████████████████▊| 3.23G/3.24G [02:25<00:00, 22.6MB/s]\n",
      "100%|██████████████████████████████████████| 3.24G/3.24G [02:25<00:00, 23.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d sanglequang/brats2018/version/1 -p /media/unet-hdd128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /media/unet-hdd128/brats2018.zip\n",
      "caution: filename not matched:  /media/unet-hdd128/\n"
     ]
    }
   ],
   "source": [
    "!unzip /media/unet-hdd128/brats2018.zip -d /media/unet-hdd128/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:            15G        299M         15G        704K        114M         15G\n",
      "Swap:            0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###============================= SETTINGS ===================================###\n",
    "DATA_SIZE = 'half' # (small, half or all)\n",
    "\n",
    "save_dir = \"/media/unet-hdd128/data_all/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "HGG_data_path = \"/media/unet-hdd128/MICCAI_BraTS_2018_Data_Training/HGG\"\n",
    "LGG_data_path = \"/media/unet-hdd128/MICCAI_BraTS_2018_Data_Training/LGG\"\n",
    "survival_csv_path = \"/media/unet-hdd128/MICCAI_BraTS_2018_Data_Training/survival_data.csv\"\n",
    "###==========================================================================###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "source": [
    "survival_id_list = []\n",
    "survival_age_list =[]\n",
    "survival_peroid_list = []\n",
    "\n",
    "with open(survival_csv_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for idx, content in enumerate(reader):\n",
    "        survival_id_list.append(content[0])\n",
    "        survival_age_list.append(float(content[1]))\n",
    "        survival_peroid_list.append(float(content[2]))\n",
    "\n",
    "print(len(survival_id_list)) #163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 30\n"
     ]
    }
   ],
   "source": [
    "if DATA_SIZE == 'all':\n",
    "    HGG_path_list = tl.files.load_folder_list(path=HGG_data_path)\n",
    "    LGG_path_list = tl.files.load_folder_list(path=LGG_data_path)\n",
    "elif DATA_SIZE == 'half':\n",
    "    HGG_path_list = tl.files.load_folder_list(path=HGG_data_path)[0:100]# DEBUG WITH SMALL DATA\n",
    "    LGG_path_list = tl.files.load_folder_list(path=LGG_data_path)[0:30] # DEBUG WITH SMALL DATA\n",
    "elif DATA_SIZE == 'small':\n",
    "    HGG_path_list = tl.files.load_folder_list(path=HGG_data_path)[0:50] # DEBUG WITH SMALL DATA\n",
    "    LGG_path_list = tl.files.load_folder_list(path=LGG_data_path)[0:20] # DEBUG WITH SMALL DATA\n",
    "elif DATA_SIZE == 'smallest':\n",
    "    HGG_path_list = tl.files.load_folder_list(path=HGG_data_path)[0:20] # DEBUG WITH SMALL DATA\n",
    "    LGG_path_list = tl.files.load_folder_list(path=LGG_data_path)[0:10] # DEBUG WITH SMALL DATA\n",
    "    \n",
    "else:\n",
    "    exit(\"Unknow DATA_SIZE\")\n",
    "print(len(HGG_path_list), len(LGG_path_list)) #210 #75\n",
    "\n",
    "HGG_name_list = [os.path.basename(p) for p in HGG_path_list]\n",
    "LGG_name_list = [os.path.basename(p) for p in LGG_path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(HGG_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brats18_TCIA08_319_1\n",
      "Brats18_TCIA08_218_1\n",
      "Brats18_TCIA08_406_1\n",
      "Brats18_TCIA06_247_1\n",
      "Brats18_TCIA06_372_1\n",
      "Brats18_TCIA06_184_1\n",
      "Brats18_TCIA04_437_1\n",
      "Brats18_TCIA04_361_1\n",
      "Brats18_TCIA04_192_1\n",
      "Brats18_TCIA04_479_1\n",
      "Brats18_TCIA04_343_1\n",
      "Brats18_TCIA03_419_1\n",
      "Brats18_TCIA03_199_1\n",
      "Brats18_TCIA03_498_1\n",
      "Brats18_TCIA03_338_1\n",
      "Brats18_TCIA03_265_1\n",
      "Brats18_TCIA03_121_1\n",
      "Brats18_TCIA02_274_1\n",
      "Brats18_TCIA02_473_1\n",
      "Brats18_TCIA02_322_1\n",
      "Brats18_TCIA02_179_1\n",
      "Brats18_TCIA02_368_1\n",
      "Brats18_TCIA02_135_1\n",
      "Brats18_TCIA02_394_1\n",
      "Brats18_TCIA02_151_1\n",
      "Brats18_TCIA02_226_1\n",
      "Brats18_TCIA02_455_1\n",
      "Brats18_TCIA02_283_1\n",
      "Brats18_TCIA02_430_1\n",
      "Brats18_TCIA02_321_1\n",
      "Brats18_TCIA02_290_1\n",
      "Brats18_TCIA02_377_1\n",
      "Brats18_TCIA02_331_1\n",
      "Brats18_TCIA02_491_1\n",
      "Brats18_TCIA01_150_1\n",
      "Brats18_TCIA01_203_1\n",
      "Brats18_TCIA01_235_1\n",
      "Brats18_TCIA01_401_1\n",
      "Brats18_TCIA01_378_1\n",
      "Brats18_TCIA01_186_1\n",
      "Brats18_TCIA01_190_1\n",
      "Brats18_2013_11_1\n",
      "Brats18_CBICA_AZD_1\n",
      "Brats18_CBICA_AYA_1\n",
      "Brats18_CBICA_AXQ_1\n",
      "Brats18_CBICA_AXL_1\n",
      "Brats18_CBICA_AWI_1\n",
      "Brats18_CBICA_AWH_1\n",
      "Brats18_CBICA_AVV_1\n",
      "Brats18_CBICA_AVJ_1\n",
      "Brats18_CBICA_AVG_1\n",
      "Brats18_CBICA_AUR_1\n",
      "Brats18_CBICA_AUQ_1\n",
      "Brats18_CBICA_AUN_1\n",
      "Brats18_CBICA_ATF_1\n",
      "Brats18_CBICA_ATD_1\n",
      "Brats18_CBICA_ASV_1\n",
      "Brats18_CBICA_ASU_1\n",
      "Brats18_CBICA_ASO_1\n",
      "Brats18_CBICA_ASN_1\n",
      "Brats18_CBICA_ASK_1\n",
      "Brats18_CBICA_ASH_1\n",
      "Brats18_CBICA_ASG_1\n",
      "Brats18_CBICA_ASE_1\n",
      "Brats18_CBICA_ASA_1\n",
      "Brats18_CBICA_AQZ_1\n",
      "Brats18_CBICA_AQY_1\n",
      "Brats18_CBICA_AQT_1\n",
      "Brats18_CBICA_AQP_1\n",
      "Brats18_CBICA_AQO_1\n",
      "Brats18_CBICA_AQJ_1\n",
      "Brats18_CBICA_AQD_1\n",
      "Brats18_CBICA_APZ_1\n",
      "Brats18_CBICA_APR_1\n",
      "Brats18_CBICA_AOP_1\n",
      "Brats18_CBICA_AOH_1\n",
      "Brats18_CBICA_AOD_1\n",
      "Brats18_CBICA_ANP_1\n",
      "Brats18_CBICA_ANG_1\n",
      "Brats18_CBICA_AMH_1\n",
      "Brats18_CBICA_AME_1\n",
      "Brats18_CBICA_ALX_1\n",
      "Brats18_CBICA_ALU_1\n",
      "Brats18_CBICA_ALN_1\n",
      "Brats18_CBICA_ABN_1\n",
      "Brats18_CBICA_AAP_1\n",
      "Brats18_CBICA_AAL_1\n",
      "Brats18_CBICA_AAB_1\n",
      "75 0\n"
     ]
    }
   ],
   "source": [
    "survival_id_from_HGG = []\n",
    "survival_id_from_LGG = []\n",
    "for i in survival_id_list:\n",
    "    if i in HGG_name_list:\n",
    "        survival_id_from_HGG.append(i)\n",
    "    elif i in LGG_name_list:\n",
    "        survival_id_from_LGG.append(i)\n",
    "    else:\n",
    "        print(i)\n",
    "\n",
    "print(len(survival_id_from_HGG), len(survival_id_from_LGG)) #163, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(survival_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use 42 from 210 (in 163 subset) and 15 from 75 as 0.8/0.2 train/dev split\n",
    "\n",
    "# use 126/42/42 from 210 (in 163 subset) and 45/15/15 from 75 as 0.6/0.2/0.2 train/dev/test split\n",
    "index_HGG = list(range(0, len(survival_id_from_HGG)))\n",
    "index_LGG = list(range(0, len(LGG_name_list)))\n",
    "# random.shuffle(index_HGG)\n",
    "# random.shuffle(index_HGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_HGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SIZE == 'all':\n",
    "    dev_index_HGG = index_HGG[-84:-42]\n",
    "    test_index_HGG = index_HGG[-42:]\n",
    "    tr_index_HGG = index_HGG[:-84]\n",
    "    dev_index_LGG = index_LGG[-30:-15]\n",
    "    test_index_LGG = index_LGG[-15:]\n",
    "    tr_index_LGG = index_LGG[:-30]\n",
    "elif DATA_SIZE == 'half':\n",
    "    dev_index_HGG = index_HGG[-30:]  # DEBUG WITH SMALL DATA\n",
    "    test_index_HGG = index_HGG[-5:]\n",
    "    tr_index_HGG = index_HGG[:-30]\n",
    "    dev_index_LGG = index_LGG[-10:]  # DEBUG WITH SMALL DATA\n",
    "    test_index_LGG = index_LGG[-5:]\n",
    "    tr_index_LGG = index_LGG[:-10]\n",
    "elif DATA_SIZE == 'small':\n",
    "    dev_index_HGG = index_HGG[35:42]   # DEBUG WITH SMALL DATA\n",
    "    # print(index_HGG, dev_index_HGG)\n",
    "    # exit()\n",
    "    test_index_HGG = index_HGG[41:42]\n",
    "    tr_index_HGG = index_HGG[0:35]\n",
    "    dev_index_LGG = index_LGG[7:10]    # DEBUG WITH SMALL DATA\n",
    "    test_index_LGG = index_LGG[9:10]\n",
    "    tr_index_LGG = index_LGG[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_index_HGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_id_dev_HGG = [survival_id_from_HGG[i] for i in dev_index_HGG]\n",
    "survival_id_test_HGG = [survival_id_from_HGG[i] for i in test_index_HGG]\n",
    "survival_id_tr_HGG = [survival_id_from_HGG[i] for i in tr_index_HGG]\n",
    "\n",
    "survival_id_dev_LGG = [LGG_name_list[i] for i in dev_index_LGG]\n",
    "survival_id_test_LGG = [LGG_name_list[i] for i in test_index_LGG]\n",
    "survival_id_tr_LGG = [LGG_name_list[i] for i in tr_index_LGG]\n",
    "\n",
    "survival_age_dev = [survival_age_list[survival_id_list.index(i)] for i in survival_id_dev_HGG]\n",
    "survival_age_test = [survival_age_list[survival_id_list.index(i)] for i in survival_id_test_HGG]\n",
    "survival_age_tr = [survival_age_list[survival_id_list.index(i)] for i in survival_id_tr_HGG]\n",
    "\n",
    "survival_period_dev = [survival_peroid_list[survival_id_list.index(i)] for i in survival_id_dev_HGG]\n",
    "survival_period_test = [survival_peroid_list[survival_id_list.index(i)] for i in survival_id_test_HGG]\n",
    "survival_period_tr = [survival_peroid_list[survival_id_list.index(i)] for i in survival_id_tr_HGG]\n",
    "\n",
    "data_types = ['flair', 't1', 't1ce', 't2']\n",
    "data_types_mean_std_dict = {i: {'mean': 0.0, 'std': 1.0} for i in data_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(survival_id_dev_HGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preserving_ratio = 0.0\n",
    "# preserving_ratio = 0.01 # 0.118 removed\n",
    "# preserving_ratio = 0.05 # 0.213 removed\n",
    "# preserving_ratio = 0.10 # 0.359 removed\n",
    "\n",
    "#==================== LOAD ALL IMAGES' PATH AND COMPUTE MEAN/ STD\n",
    "for i in data_types:\n",
    "    data_temp_list = []\n",
    "    for j in HGG_name_list:\n",
    "        img_path = os.path.join(HGG_data_path, j, j + '_' + i + '.nii.gz')\n",
    "        img = nib.load(img_path).get_data()\n",
    "        data_temp_list.append(img)\n",
    "\n",
    "    for j in LGG_name_list:\n",
    "        img_path = os.path.join(LGG_data_path, j, j + '_' + i + '.nii.gz')\n",
    "        img = nib.load(img_path).get_data()\n",
    "        data_temp_list.append(img)\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "    data_temp_list = np.asarray(data_temp_list)\n",
    "    m = np.mean(data_temp_list)\n",
    "    s = np.std(data_temp_list)\n",
    "    data_types_mean_std_dict[i]['mean'] = m\n",
    "    data_types_mean_std_dict[i]['std'] = s\n",
    "del data_temp_list\n",
    "print(data_types_mean_std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types_mean_std_dict = {'flair': {'mean': 55.052796611352356, 'std': 380.9875011565797},\n",
    " 't1': {'mean': 71.32039103770333, 'std': 366.6561974269796},\n",
    " 't1ce': {'mean': 80.16341578870278, 'std': 451.33543035257645},\n",
    " 't2': {'mean': 77.49268785239178, 'std': 443.6208077152802}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flair': {'mean': 55.052796611352356, 'std': 380.9875011565797},\n",
       " 't1': {'mean': 71.32039103770333, 'std': 366.6561974269796},\n",
       " 't1ce': {'mean': 80.16341578870278, 'std': 451.33543035257645},\n",
       " 't2': {'mean': 77.49268785239178, 'std': 443.6208077152802}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_types_mean_std_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_types_mean_std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##==================== GET NORMALIZE IMAGES\n",
    "X_train_input = []\n",
    "X_train_target = []\n",
    "# X_train_target_whole = [] # 1 2 4\n",
    "# X_train_target_core = [] # 1 4\n",
    "# X_train_target_enhance = [] # 4\n",
    "\n",
    "X_dev_input = []\n",
    "X_dev_target = []\n",
    "# X_dev_target_whole = [] # 1 2 4x\n",
    "# X_dev_target_core = [] # 1 4\n",
    "# X_dev_target_enhance = [] # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HGG Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n",
      "/home/azureuser/.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished Brats18_CBICA_AXM_1\n",
      "finished Brats18_CBICA_AXJ_1\n",
      "finished Brats18_CBICA_AWG_1\n",
      "finished Brats18_CBICA_ATX_1\n",
      "finished Brats18_CBICA_ATV_1\n",
      "finished Brats18_CBICA_ATP_1\n",
      "finished Brats18_CBICA_ATB_1\n",
      "finished Brats18_CBICA_ASY_1\n",
      "finished Brats18_CBICA_ASW_1\n",
      "finished Brats18_CBICA_ARZ_1\n",
      "finished Brats18_CBICA_ARW_1\n",
      "finished Brats18_CBICA_ARF_1\n",
      "finished Brats18_CBICA_AQV_1\n",
      "finished Brats18_CBICA_AQU_1\n",
      "finished Brats18_CBICA_AQR_1\n",
      "finished Brats18_CBICA_AQQ_1\n",
      "finished Brats18_CBICA_AQN_1\n",
      "finished Brats18_CBICA_AQG_1\n",
      "finished Brats18_CBICA_AQA_1\n",
      "finished Brats18_CBICA_APY_1\n",
      "finished Brats18_CBICA_AOZ_1\n",
      "finished Brats18_CBICA_AOO_1\n",
      "finished Brats18_CBICA_ANZ_1\n",
      "finished Brats18_CBICA_ANI_1\n",
      "finished Brats18_CBICA_ABY_1\n",
      "finished Brats18_CBICA_ABO_1\n",
      "finished Brats18_CBICA_ABM_1\n",
      "finished Brats18_CBICA_ABE_1\n",
      "finished Brats18_CBICA_ABB_1\n",
      "finished Brats18_CBICA_AAG_1\n"
     ]
    }
   ],
   "source": [
    "print(\" HGG Validation\")\n",
    "for i in survival_id_dev_HGG:\n",
    "    all_3d_data = []\n",
    "    for j in data_types:\n",
    "        img_path = os.path.join(HGG_data_path, i, i + '_' + j + '.nii')\n",
    "        img = nib.load(img_path).get_data()\n",
    "        img = (img - data_types_mean_std_dict[j]['mean']) / data_types_mean_std_dict[j]['std']\n",
    "        img = img.astype(np.float32)\n",
    "        all_3d_data.append(img)\n",
    "\n",
    "    seg_path = os.path.join(HGG_data_path, i, i + '_seg.nii')\n",
    "    seg_img = nib.load(seg_path).get_data()\n",
    "    seg_img = np.transpose(seg_img, (1, 0, 2))\n",
    "    for j in range(all_3d_data[0].shape[2]):\n",
    "        combined_array = np.stack((all_3d_data[0][:, :, j], all_3d_data[1][:, :, j], all_3d_data[2][:, :, j], all_3d_data[3][:, :, j]), axis=2)\n",
    "        combined_array = np.transpose(combined_array, (1, 0, 2))#.tolist()\n",
    "        combined_array.astype(np.float32)\n",
    "        X_dev_input.append(combined_array)\n",
    "\n",
    "        seg_2d = seg_img[:, :, j]\n",
    "        # whole = np.zeros_like(seg_2d)\n",
    "        # core = np.zeros_like(seg_2d)\n",
    "        # enhance = np.zeros_like(seg_2d)\n",
    "        # for index, x in np.ndenumerate(seg_2d):\n",
    "        #     if x == 1:\n",
    "        #         whole[index] = 1\n",
    "        #         core[index] = 1\n",
    "        #     if x == 2:\n",
    "        #         whole[index] = 1\n",
    "        #     if x == 4:\n",
    "        #         whole[index] = 1\n",
    "        #         core[index] = 1\n",
    "        #         enhance[index] = 1\n",
    "        # X_dev_target_whole.append(whole)\n",
    "        # X_dev_target_core.append(core)\n",
    "        # X_dev_target_enhance.append(enhance)\n",
    "        seg_2d.astype(int)\n",
    "        X_dev_target.append(seg_2d)\n",
    "    del all_3d_data\n",
    "    gc.collect()\n",
    "    print(\"finished {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell\n"
     ]
    }
   ],
   "source": [
    "print('hell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LGG Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n",
      "/home/azureuser/.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished Brats18_TCIA13_621_1\n",
      "finished Brats18_TCIA10_103_1\n",
      "finished Brats18_TCIA13_615_1\n",
      "finished Brats18_2013_9_1\n",
      "finished Brats18_TCIA10_632_1\n",
      "finished Brats18_TCIA10_442_1\n",
      "finished Brats18_TCIA13_642_1\n",
      "finished Brats18_2013_6_1\n",
      "finished Brats18_2013_29_1\n",
      "finished Brats18_TCIA09_255_1\n"
     ]
    }
   ],
   "source": [
    "print(\" LGG Validation\")\n",
    "for i in survival_id_dev_LGG:\n",
    "    all_3d_data = []\n",
    "    for j in data_types:\n",
    "        img_path = os.path.join(LGG_data_path, i, i + '_' + j + '.nii')\n",
    "        img = nib.load(img_path).get_data()\n",
    "        img = (img - data_types_mean_std_dict[j]['mean']) / data_types_mean_std_dict[j]['std']\n",
    "        img = img.astype(np.float32)\n",
    "        all_3d_data.append(img)\n",
    "\n",
    "    seg_path = os.path.join(LGG_data_path, i, i + '_seg.nii')\n",
    "    seg_img = nib.load(seg_path).get_data()\n",
    "    seg_img = np.transpose(seg_img, (1, 0, 2))\n",
    "    for j in range(all_3d_data[0].shape[2]):\n",
    "        combined_array = np.stack((all_3d_data[0][:, :, j], all_3d_data[1][:, :, j], all_3d_data[2][:, :, j], all_3d_data[3][:, :, j]), axis=2)\n",
    "        combined_array = np.transpose(combined_array, (1, 0, 2))#.tolist()\n",
    "        combined_array.astype(np.float32)\n",
    "        X_dev_input.append(combined_array)\n",
    "\n",
    "        seg_2d = seg_img[:, :, j]\n",
    "        # whole = np.zeros_like(seg_2d)\n",
    "        # core = np.zeros_like(seg_2d)\n",
    "        # enhance = np.zeros_like(seg_2d)\n",
    "        # for index, x in np.ndenumerate(seg_2d):\n",
    "        #     if x == 1:\n",
    "        #         whole[index] = 1\n",
    "        #         core[index] = 1\n",
    "        #     if x == 2:\n",
    "        #         whole[index] = 1\n",
    "        #     if x == 4:\n",
    "        #         whole[index] = 1\n",
    "        #         core[index] = 1\n",
    "        #         enhance[index] = 1\n",
    "        # X_dev_target_whole.append(whole)\n",
    "        # X_dev_target_core.append(core)\n",
    "        # X_dev_target_enhance.append(enhance)\n",
    "        seg_2d.astype(int)\n",
    "        X_dev_target.append(seg_2d)\n",
    "    del all_3d_data\n",
    "    gc.collect()\n",
    "    print(\"finished {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_input = np.asarray(X_dev_input, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 240, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_input[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 240, 240, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_input[20:40,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "# define data\n",
    "batch_size = 10\n",
    "for i in range(0, X_dev_input.shape[0], batch_size):\n",
    "    path = \"X_dev_input_batch_{}.npy\".format(i)\n",
    "    # save to npy file\n",
    "    save(save_dir  + \"X_dev_intput/\" path, X_dev_input[i:i+batch_size,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_dev_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_target = np.asarray(X_dev_target)#, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "# define data\n",
    "batch_size = 10\n",
    "for i in range(0, X_dev_target.shape[0], batch_size):\n",
    "    path = \"X_dev_target_batch_{}.npy\".format(i)\n",
    "    # save to npy file\n",
    "    save(save_dir + \"X_dev_target/\" + path, X_dev_target[i:i+batch_size,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_dev_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_dir + 'train_dev_input_half.pickle', 'wb') as f:\n",
    "#     pickle.dump(X_dev_input, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_dir + 'train_dev_target_half.pickle', 'wb') as f:\n",
    "#     pickle.dump(X_dev_target, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HGG Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n",
      "/home/azureuser/.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished Brats18_TCIA08_167_1\n",
      "155\n",
      "finished Brats18_TCIA08_242_1\n",
      "310\n",
      "finished Brats18_TCIA08_469_1\n",
      "465\n",
      "finished Brats18_TCIA08_280_1\n",
      "620\n",
      "finished Brats18_TCIA08_105_1\n",
      "775\n",
      "finished Brats18_TCIA08_278_1\n",
      "930\n",
      "finished Brats18_TCIA06_165_1\n",
      "1085\n",
      "finished Brats18_TCIA06_409_1\n",
      "1240\n",
      "finished Brats18_TCIA05_277_1\n",
      "1395\n",
      "finished Brats18_TCIA05_478_1\n",
      "1550\n",
      "finished Brats18_TCIA04_111_1\n",
      "1705\n",
      "finished Brats18_TCIA04_149_1\n",
      "1860\n",
      "finished Brats18_TCIA03_474_1\n",
      "2015\n",
      "finished Brats18_TCIA03_133_1\n",
      "2170\n",
      "finished Brats18_TCIA03_296_1\n",
      "2325\n",
      "finished Brats18_TCIA03_257_1\n",
      "2480\n",
      "finished Brats18_TCIA03_138_1\n",
      "2635\n",
      "finished Brats18_TCIA03_375_1\n",
      "2790\n",
      "finished Brats18_TCIA02_471_1\n",
      "2945\n",
      "finished Brats18_TCIA02_300_1\n",
      "3100\n",
      "finished Brats18_TCIA02_118_1\n",
      "3255\n",
      "finished Brats18_TCIA02_314_1\n",
      "3410\n",
      "finished Brats18_TCIA02_198_1\n",
      "3565\n",
      "finished Brats18_TCIA01_335_1\n",
      "3720\n",
      "finished Brats18_TCIA01_411_1\n",
      "3875\n",
      "finished Brats18_TCIA01_231_1\n",
      "4030\n",
      "finished Brats18_TCIA01_390_1\n",
      "4185\n",
      "finished Brats18_TCIA01_499_1\n",
      "4340\n",
      "finished Brats18_TCIA01_412_1\n",
      "4495\n",
      "finished Brats18_TCIA01_448_1\n",
      "4650\n",
      "finished Brats18_TCIA01_147_1\n",
      "4805\n",
      "finished Brats18_TCIA01_201_1\n",
      "4960\n",
      "finished Brats18_TCIA01_429_1\n",
      "5115\n",
      "finished Brats18_TCIA01_460_1\n",
      "5270\n",
      "finished Brats18_TCIA01_425_1\n",
      "5425\n",
      "finished Brats18_2013_27_1\n",
      "5580\n",
      "finished Brats18_CBICA_BHM_1\n",
      "5735\n",
      "finished Brats18_CBICA_BHB_1\n",
      "5890\n",
      "finished Brats18_CBICA_AZH_1\n",
      "6045\n",
      "finished Brats18_CBICA_AYW_1\n",
      "6200\n",
      "finished Brats18_CBICA_AYU_1\n",
      "6355\n",
      "finished Brats18_CBICA_AYI_1\n",
      "6510\n",
      "finished Brats18_CBICA_AXW_1\n",
      "6665\n",
      "finished Brats18_CBICA_AXO_1\n",
      "6820\n",
      "finished Brats18_CBICA_AXN_1\n",
      "6975\n"
     ]
    }
   ],
   "source": [
    "print(\" HGG Train\")\n",
    "for i in survival_id_tr_HGG:\n",
    "    all_3d_data = []\n",
    "    for j in data_types:\n",
    "        img_path = os.path.join(HGG_data_path, i, i + '_' + j + '.nii')\n",
    "        img = nib.load(img_path).get_data()\n",
    "        img = (img - data_types_mean_std_dict[j]['mean']) / data_types_mean_std_dict[j]['std']\n",
    "        img = img.astype(np.float32)\n",
    "        all_3d_data.append(img)\n",
    "\n",
    "    seg_path = os.path.join(HGG_data_path, i, i + '_seg.nii')\n",
    "    seg_img = nib.load(seg_path).get_data()\n",
    "    seg_img = np.transpose(seg_img, (1, 0, 2))\n",
    "    for j in range(all_3d_data[0].shape[2]):\n",
    "        combined_array = np.stack((all_3d_data[0][:, :, j], all_3d_data[1][:, :, j], all_3d_data[2][:, :, j], all_3d_data[3][:, :, j]), axis=2)\n",
    "        combined_array = np.transpose(combined_array, (1, 0, 2))#.tolist()\n",
    "        combined_array.astype(np.float32)\n",
    "        X_train_input.append(combined_array)\n",
    "\n",
    "        seg_2d = seg_img[:, :, j]\n",
    "        # whole = np.zeros_like(seg_2d)\n",
    "        # core = np.zeros_like(seg_2d)\n",
    "        # enhance = np.zeros_like(seg_2d)\n",
    "        # for index, x in np.ndenumerate(seg_2d):\n",
    "        #     if x == 1:\n",
    "        #         whole[index] = 1\n",
    "        #         core[index] = 1\n",
    "        #     if x == 2:\n",
    "        #         whole[index] = 1\n",
    "        #     if x == 4:\n",
    "        #         whole[index] = 1\n",
    "        #         core[index] = 1\n",
    "        #         enhance[index] = 1\n",
    "        # X_train_target_whole.append(whole)\n",
    "        # X_train_target_core.append(core)\n",
    "        # X_train_target_enhance.append(enhance)\n",
    "        seg_2d.astype(int)\n",
    "        X_train_target.append(seg_2d)\n",
    "    del all_3d_data\n",
    "    print(\"finished {}\".format(i))\n",
    "    print(len(X_train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LGG Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n",
      "/home/azureuser/.pyenv/versions/3.6.9/envs/unet-env-3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished Brats18_TCIA12_466_1\n",
      "finished Brats18_TCIA09_462_1\n",
      "finished Brats18_TCIA10_310_1\n",
      "finished Brats18_TCIA10_109_1\n",
      "finished Brats18_TCIA10_299_1\n",
      "finished Brats18_TCIA10_241_1\n",
      "finished Brats18_TCIA10_629_1\n",
      "finished Brats18_TCIA13_653_1\n",
      "finished Brats18_2013_15_1\n",
      "finished Brats18_TCIA09_493_1\n",
      "finished Brats18_TCIA09_620_1\n",
      "finished Brats18_TCIA10_413_1\n",
      "finished Brats18_TCIA13_645_1\n",
      "finished Brats18_2013_0_1\n",
      "finished Brats18_TCIA10_282_1\n",
      "finished Brats18_TCIA09_177_1\n",
      "finished Brats18_2013_24_1\n",
      "finished Brats18_TCIA10_130_1\n",
      "finished Brats18_TCIA10_346_1\n",
      "finished Brats18_TCIA10_639_1\n"
     ]
    }
   ],
   "source": [
    "print(\" LGG Train\")\n",
    "for i in survival_id_tr_LGG:\n",
    "    all_3d_data = []\n",
    "    for j in data_types:\n",
    "        img_path = os.path.join(LGG_data_path, i, i + '_' + j + '.nii')\n",
    "        img = nib.load(img_path).get_data()\n",
    "        img = (img - data_types_mean_std_dict[j]['mean']) / data_types_mean_std_dict[j]['std']\n",
    "        img = img.astype(np.float32)\n",
    "        all_3d_data.append(img)\n",
    "\n",
    "    seg_path = os.path.join(LGG_data_path, i, i + '_seg.nii')\n",
    "    seg_img = nib.load(seg_path).get_data()\n",
    "    seg_img = np.transpose(seg_img, (1, 0, 2))\n",
    "    for j in range(all_3d_data[0].shape[2]):\n",
    "        combined_array = np.stack((all_3d_data[0][:, :, j], all_3d_data[1][:, :, j], all_3d_data[2][:, :, j], all_3d_data[3][:, :, j]), axis=2)\n",
    "        combined_array = np.transpose(combined_array, (1, 0, 2))#.tolist()\n",
    "        combined_array.astype(np.float32)\n",
    "        X_train_input.append(combined_array)\n",
    "\n",
    "        seg_2d = seg_img[:, :, j]\n",
    "        # whole = np.zeros_like(seg_2d)\n",
    "        # core = np.zeros_like(seg_2d)\n",
    "        # enhance = np.zeros_like(seg_2d)\n",
    "        # for index, x in np.ndenumerate(seg_2d):\n",
    "        #     if x == 1:\n",
    "        #         whole[index] = 1\n",
    "        #         core[index] = 1\n",
    "        #     if x == 2:\n",
    "        #         whole[index] = 1\n",
    "        #     if x == 4:\n",
    "        #         whole[index] = 1\n",
    "        #         core[index] = 1\n",
    "        #         enhance[index] = 1\n",
    "        # X_train_target_whole.append(whole)\n",
    "        # X_train_target_core.append(core)\n",
    "        # X_train_target_enhance.append(enhance)\n",
    "        seg_2d.astype(int)\n",
    "        X_train_target.append(seg_2d)\n",
    "    del all_3d_data\n",
    "    print(\"finished {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10075"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_input[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(X_train_input[5:10], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 240, 240, 4)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "# define data\n",
    "batch_size = 10\n",
    "for i in range(0, len(X_train_input), batch_size):\n",
    "    path = \"X_train_input_batch_{}.npy\".format(i)\n",
    "    data = np.asarray(X_train_input[i:i+batch_size], dtype=np.float32)\n",
    "    # save to npy file\n",
    "    save(save_dir + \"X_train_input/\" + path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "# define data\n",
    "batch_size = 10\n",
    "for i in range(0, len(X_train_target), batch_size):\n",
    "    path = \"X_train_target_batch_{}.npy\".format(i)\n",
    "    data = np.asarray(X_train_target[i:i+batch_size], dtype=np.float32)\n",
    "    # save to npy file\n",
    "    save(save_dir + \"X_train_target/\" + path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_dir + 'train_input_half.pickle', 'wb') as f:\n",
    "#     pickle.dump(X_train_input, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_dir + 'train_target.pickle', 'wb') as f:\n",
    "#     pickle.dump(X_train_target, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
